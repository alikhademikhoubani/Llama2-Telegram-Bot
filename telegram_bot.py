# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u5P_eA-WUDLhsA7xzKYBtBm-4Jz8h7YS
"""

!pip install -U sacremoses

!pip install -U bitsandbytes

!pip3 install psutil

!pip3 install pyTelegramBotAPI

!pip install -q bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline
import torch
import telebot
import psutil
import os
import time

mname = 'NousResearch/Llama-2-7b-chat-hf'

tokenizer = AutoTokenizer.from_pretrained(mname, trust_remote_code = True, use_fast = False)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'

bnb_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_compute_dtype = torch.bfloat16,
    bnb_4bit_use_double_quant = False
)

model = AutoModelForCausalLM.from_pretrained(
    mname,
    quantization_config = bnb_config,
    device_map = {"": 0}
)

token = '7613613250:AAGshq3K0Ub1pwIHslqHONTRqT5RefRnMZU'

bot = telebot.TeleBot(token)

@bot.message_handler(command=['start'])
def start_message(message):
  bot.send_message(message.chat.id, 'hi')

@bot.message_handler(content_types=['text'])
def send_text(message):
  action_function(message)

def text_generation(prompt):
    pipe = pipeline(task = 'text-generation', model = model, tokenizer = tokenizer, max_length = 100)
    result = pipe(f'<s>[INST] {prompt} [/INST]')
    print(result[0]['generated_text'])
    s = result[0]['generated_text']
    d = s.split('[/INST]')
    return d[1]

text_generation('hi')

def action_function(message):
  if message.text.lower():
    input = message.text
    outputs = text_generation(input)
    bot.send_message(message.chat.id, outputs)

if __name__=='__main__':
  while True:
    try:
      bot.polling(non_stop=True, interval=0)
    except Exception as e:
      print(e)
      time.sleep(5)
      continue

